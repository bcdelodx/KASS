# KASS Real Data Remediation Plan
## Response to Institutional Audit Report (2026-01-13)

**Plan Date:** 2026-01-13
**Audit Grade:** B (83/100)
**Target Grade:** A (95+/100)
**Timeline:** 6-12 months
**Critical Deficiency:** Data Authenticity (20/100)

---

## I. EXECUTIVE SUMMARY

This plan responds to the critical data authenticity deficiency identified in the institutional audit. We will systematically replace simulated data with real data from authoritative sources using the KRL data connectors infrastructure.

### Key Strategy
- **Phase 1 (Months 1-3):** Quick wins with public APIs (NB07)
- **Phase 2 (Months 4-6):** Medium-complexity data access (NB20)
- **Phase 3 (Months 7-12):** Administrative data with DUAs (NB22)

---

## II. IMMEDIATE ACTIONS (Next 30 Days)

### A. Repository Reorganization ‚úÖ
```
KASS/
‚îú‚îÄ‚îÄ production/           # Real data, peer-review ready
‚îú‚îÄ‚îÄ templates/            # Methodology demonstrations (current notebooks)
‚îú‚îÄ‚îÄ development/          # Works in progress
‚îî‚îÄ‚îÄ data_connectors/      # KRL connector integration
```

### B. Add Warning Badges to Existing Notebooks
- Add "METHODOLOGICAL TEMPLATE" to all notebook titles
- Insert "Production Data Requirements" section at top
- Add "DO NOT CITE SIMULATED EFFECTS" disclaimer
- Watermark all plots with "SIMULATED DATA"

### C. Data Connector Infrastructure Setup
- Install and test krl-data-connectors package
- Configure API credentials (BLS, FRED, Census)
- Create connector utility module for KASS
- Document data fetching patterns

---

## III. PRIORITY 1: NB07 LABOR MARKET INTELLIGENCE
**Target Grade:** A- (92/100)
**Timeline:** Months 1-3
**Complexity:** LOW (Public APIs only)

### Current State
- ‚úÖ Real FRED macro data (national/state level)
- ‚ùå Simulated metro-level employment outcomes
- ‚ùå Synthetic industry-level variation

### Real Data Sources via KRL Connectors

#### 1. BLS QCEW (Quarterly Census of Employment and Wages)
**Connector:** `krl_data_connectors.bls.QCEWConnector`

```python
from krl_data_connectors.bls import QCEWConnector

qcew = QCEWConnector(api_key=os.getenv("BLS_API_KEY"))

# MSA-level employment by industry
msa_employment = qcew.get_msa_data(
    area_codes=["C1692", "C1602", "C3536"],  # Boston, Atlanta, Dallas
    industry_codes=["10", "1012", "1013"],    # Total, Construction, Manufacturing
    years=range(2015, 2024),
    frequency="quarterly"
)
```

**Data Coverage:**
- 400+ Metropolitan Statistical Areas
- NAICS industry detail (2-6 digit)
- Quarterly frequency, 2010-present
- Employment, wages, establishments

#### 2. BLS LAUS (Local Area Unemployment Statistics)
**Connector:** `krl_data_connectors.bls.LAUSConnector`

```python
from krl_data_connectors.bls import LAUSConnector

laus = LAUSConnector(api_key=os.getenv("BLS_API_KEY"))

# MSA unemployment rates
msa_unemployment = laus.get_area_data(
    area_type="MSA",
    area_codes=["LAUCN420033", "LAUCN131300"],  # Pittsburgh, Atlanta MSA
    measures=["unemployment_rate", "labor_force"],
    start_year=2015,
    end_year=2023
)
```

#### 3. Census LEHD (Job Flows)
**Connector:** `krl_data_connectors.census.LEHDConnector`

```python
from krl_data_connectors.census import LEHDConnector

lehd = LEHDConnector(api_key=os.getenv("CENSUS_API_KEY"))

# Quarterly Workforce Indicators by MSA
qwi = lehd.get_qwi(
    geography="MSA",
    codes=["14460", "12060"],  # Boston, Atlanta
    indicators=["Emp", "EmpTotal", "HirA", "Sep"],
    years=range(2015, 2023)
)
```

### Implementation Plan

**Week 1-2: Data Acquisition**
- Set up BLS/Census API credentials
- Test all connectors with sample requests
- Build data caching layer
- Document rate limits and quotas

**Week 3-4: Data Integration**
- Replace simulated metro employment with QCEW data
- Integrate LAUS unemployment rates
- Add LEHD job flows for dynamics
- Validate data consistency

**Week 5-6: Analysis Update**
- Re-run all econometric models
- Update visualizations
- Verify statistical inference
- Compare results to simulation

**Week 7-8: Quality Assurance**
- Peer review updated notebook
- Validate data provenance
- Test reproducibility
- Update documentation

**Week 9-12: Documentation & Testing**
- Create comprehensive data documentation
- Add data quality checks
- Implement automated tests
- Write "Real Data Migration" guide

### Expected Outcomes
- ‚úÖ 100% real employment data (no simulation)
- ‚úÖ MSA-level granularity maintained
- ‚úÖ Industry-level detail from NAICS codes
- ‚úÖ Grade improvement: B+ ‚Üí A- (92/100)

---

## IV. PRIORITY 2: NB20 OPPORTUNITY ZONE EVALUATION
**Target Grade:** A (95/100)
**Timeline:** Months 4-6
**Complexity:** MEDIUM (Mix of public + commercial data)

### Current State
- ‚úÖ Real FRED state unemployment data
- ‚ùå Simulated OZ designation (not actual Treasury list)
- ‚ùå Simulated home value outcomes

### Real Data Sources

#### 1. Treasury OZ Designation Data
**Source:** U.S. Treasury CDFI Fund Public Data
**Connector:** `krl_data_connectors.treasury.OpportunityZoneConnector`

```python
from krl_data_connectors.treasury import OpportunityZoneConnector

oz = OpportunityZoneConnector()

# Official OZ designations
oz_tracts = oz.get_designated_tracts(
    states=["CA", "TX", "NY"],
    designation_year=2018,
    include_eligibility_criteria=True
)

# Low-Income Community criteria
lic_data = oz.get_eligibility_data(
    tracts=oz_tracts["tract_id"].tolist(),
    metrics=["poverty_rate", "median_income", "tract_qualification"]
)
```

**Data Coverage:**
- 8,764 designated Opportunity Zones (all states)
- Tract-level designation status
- Eligibility criteria (LIC qualification)
- Adjacent low-income tracts

#### 2. Zillow Home Value Index (ZHVI)
**Source:** Zillow Research Data
**Connector:** `krl_data_connectors.zillow.ZHVIConnector`

```python
from krl_data_connectors.zillow import ZHVIConnector

zhvi = ZHVIConnector()

# Tract-level home values (if available via Zillow API)
# Alternative: ZIP code level aggregation
home_values = zhvi.get_home_values(
    region_type="zip",  # or "city", "county"
    regions=["90210", "10001", "75201"],
    date_range=("2015-01", "2023-12"),
    value_type="median"
)
```

**Note:** Zillow provides ZIP-level data publicly. Tract-level requires:
- CoreLogic subscription (commercial)
- Or: Aggregate ZIP ‚Üí Tract using Census crosswalks

#### 3. American Community Survey (ACS)
**Connector:** `krl_data_connectors.census.ACSConnector`

```python
from krl_data_connectors.census import ACSConnector

acs = ACSConnector(api_key=os.getenv("CENSUS_API_KEY"))

# Tract-level demographics and economics
tract_data = acs.get_tract_data(
    states=["06", "48", "36"],  # CA, TX, NY
    years=range(2013, 2022),  # 5-year estimates
    variables=[
        "B19013_001E",  # Median household income
        "B25077_001E",  # Median home value
        "B25003_002E",  # Owner-occupied units
        "B17001_002E",  # Poverty count
    ]
)
```

### Implementation Plan

**Month 4:**
- Acquire Treasury OZ designation list
- Set up Zillow/ACS data access
- Build tract-level dataset
- Create treatment/control matching algorithm

**Month 5:**
- Implement difference-in-differences with real data
- Re-run parallel trends tests
- Update all visualizations
- Validate causal estimates

**Month 6:**
- Sensitivity analysis with real data
- Peer review and quality assurance
- Update documentation
- Prepare for publication

### Data Challenges & Mitigations

**Challenge 1:** Zillow data not available at tract level
- **Mitigation:** Use ZIP-to-tract crosswalk with area weighting
- **Alternative:** Use ACS median home values (less frequent)

**Challenge 2:** Pre-treatment data limited (OZ began 2018)
- **Mitigation:** Use 2010-2017 data for parallel trends
- **Alternative:** Synthetic control matching on pre-2018

**Challenge 3:** Spillover effects across tract boundaries
- **Mitigation:** Buffer analysis (exclude adjacent tracts)
- **Alternative:** Spatial DiD with geographic controls

---

## V. PRIORITY 3: NB22 WORKFORCE DEVELOPMENT ROI
**Target Grade:** A (95/100)
**Timeline:** Months 7-12
**Complexity:** HIGH (Administrative data, DUA required)

### Current State
- ‚ùå 100% simulated participant data
- ‚ùå Pre-programmed treatment effects
- ‚ùå Synthetic earnings outcomes

### Real Data Sources

#### 1. State PIRL (Participant Individual Record Layout)
**Source:** State Workforce Agencies
**Access:** Data Use Agreement (DUA) required

**Data Elements:**
- Participant demographics (age, gender, race, education)
- Program enrollment and completion
- Training type and duration
- Pre/post-program earnings (UI wage records)
- Employment outcomes (6mo, 12mo)

**States to Target:**
- **Tier 1:** California, Texas, New York (large programs)
- **Tier 2:** Massachusetts, Washington (strong data systems)
- **Tier 3:** Multi-state aggregates via USDOL

#### 2. USDOL WIOA Performance Data
**Source:** U.S. Department of Labor
**Connector:** `krl_data_connectors.dol.WIOAConnector`

```python
from krl_data_connectors.dol import WIOAConnector

wioa = WIOAConnector()

# Public-use WIOA performance by state
state_performance = wioa.get_state_performance(
    states=["CA", "TX", "NY"],
    program_years=range(2016, 2023),
    measures=[
        "employment_rate_q2",
        "employment_rate_q4",
        "median_earnings",
        "credential_attainment"
    ]
)
```

**Data Coverage:**
- State-level aggregates (public)
- Performance measures by program type
- No individual-level data (privacy protected)

#### 3. State UI Wage Records (LEHD)
**Source:** Census LEHD via RDC
**Access:** Federal Statistical Research Data Center

**Data Elements:**
- Quarterly earnings by individual
- Employer NAICS industry
- Job start/end dates
- Multi-state employment

### Implementation Plan

**Months 7-8: Data Access Negotiation**
- Identify target state workforce agencies
- Draft and submit DUA applications
- Engage with state data officers
- Prepare IRB protocols if needed

**Months 9-10: Data Acquisition**
- Receive and ingest state PIRL data
- Link to USDOL performance benchmarks
- Build secure data environment
- Implement privacy protections

**Months 11-12: Analysis & Validation**
- Re-estimate treatment effects with real data
- Compare to simulation calibration
- Update ROI calculations
- Peer review and publication

### Data Access Strategy

**Path A: Direct State Partnership** (Preferred)
- Contact California EDD, Texas TWC, or NY DOL
- Propose research collaboration
- Offer evaluation services in exchange for data access
- Timeline: 3-6 months

**Path B: USDOL Public-Use Files** (Fallback)
- Use aggregate state-level performance data
- Supplement with CPS microdata for demographics
- Less granular but faster access
- Timeline: 1 month

**Path C: Research Data Center** (Long-term)
- Apply for Census RDC access
- Use LEHD microdata with UI records
- Most comprehensive but slowest
- Timeline: 6-12 months

---

## VI. DATA CONNECTOR ARCHITECTURE

### Central Data Management Module

Create `KASS/data_connectors/kass_data_manager.py`:

```python
"""
KASS Data Manager
Centralized interface to KRL data connectors with caching and validation
"""

from typing import Dict, List, Optional, Union
import pandas as pd
from pathlib import Path
import logging

from krl_data_connectors.bls import QCEWConnector, LAUSConnector
from krl_data_connectors.census import ACSConnector, LEHDConnector
from krl_data_connectors.fred import FREDConnector
from krl_data_connectors.treasury import OpportunityZoneConnector
from krl_data_connectors.zillow import ZHVIConnector

class KASSDataManager:
    """
    Unified data fetching and caching for KASS notebooks

    Features:
    - Automatic caching to avoid re-fetching
    - Data provenance tracking
    - Validation and quality checks
    - Consistent error handling
    """

    def __init__(self, cache_dir: Path = Path("data/cache")):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Initialize connectors
        self.fred = FREDConnector(api_key=os.getenv("FRED_API_KEY"))
        self.bls_qcew = QCEWConnector(api_key=os.getenv("BLS_API_KEY"))
        self.bls_laus = LAUSConnector(api_key=os.getenv("BLS_API_KEY"))
        self.census_acs = ACSConnector(api_key=os.getenv("CENSUS_API_KEY"))
        self.census_lehd = LEHDConnector(api_key=os.getenv("CENSUS_API_KEY"))

        # Provenance tracking
        self.provenance = {}

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def get_msa_employment(
        self,
        msa_codes: List[str],
        industry_codes: List[str],
        years: range,
        force_refresh: bool = False
    ) -> pd.DataFrame:
        """
        Fetch MSA-level employment from BLS QCEW

        Args:
            msa_codes: List of MSA FIPS codes
            industry_codes: NAICS industry codes
            years: Range of years to fetch
            force_refresh: Skip cache and fetch fresh data

        Returns:
            DataFrame with columns: msa, industry, year, quarter, employment, wages
        """
        cache_key = f"msa_employment_{'_'.join(msa_codes)}_{years.start}_{years.stop}"
        cache_file = self.cache_dir / f"{cache_key}.parquet"

        if not force_refresh and cache_file.exists():
            self.logger.info(f"Loading cached data: {cache_file}")
            data = pd.read_parquet(cache_file)
        else:
            self.logger.info(f"Fetching MSA employment from BLS QCEW")
            data = self.bls_qcew.get_msa_data(
                area_codes=msa_codes,
                industry_codes=industry_codes,
                years=years,
                frequency="quarterly"
            )
            data.to_parquet(cache_file)

        # Track provenance
        self.provenance[cache_key] = {
            "source": "BLS QCEW",
            "connector": "krl_data_connectors.bls.QCEWConnector",
            "timestamp": pd.Timestamp.now(),
            "cache_file": str(cache_file),
            "validation_passed": self._validate_employment_data(data)
        }

        return data

    def get_opportunity_zone_data(
        self,
        states: List[str],
        years: range,
        include_outcomes: bool = True
    ) -> Dict[str, pd.DataFrame]:
        """
        Fetch comprehensive Opportunity Zone data

        Returns:
            Dictionary with keys:
            - "designations": Official OZ tract list
            - "home_values": Zillow/ACS property values
            - "demographics": ACS tract characteristics
        """
        data = {}

        # Treasury designations
        oz_connector = OpportunityZoneConnector()
        data["designations"] = oz_connector.get_designated_tracts(
            states=states,
            designation_year=2018,
            include_eligibility_criteria=True
        )

        # Home values (ZIP level from Zillow, aggregated to tracts)
        if include_outcomes:
            # Get ZIP codes for OZ tracts
            zip_codes = self._get_zip_codes_for_tracts(
                data["designations"]["tract_id"].tolist()
            )

            zhvi = ZHVIConnector()
            data["home_values"] = zhvi.get_home_values(
                region_type="zip",
                regions=zip_codes,
                date_range=(f"{years.start}-01", f"{years.stop-1}-12"),
                value_type="median"
            )

            # ACS demographics
            data["demographics"] = self.census_acs.get_tract_data(
                states=[str(s).zfill(2) for s in states],
                years=list(years),
                variables=[
                    "B19013_001E",  # Median income
                    "B25077_001E",  # Median home value
                    "B17001_002E",  # Poverty count
                ]
            )

        # Provenance
        self.provenance["opportunity_zones"] = {
            "sources": ["Treasury CDFI", "Zillow ZHVI", "Census ACS"],
            "treatment_assignment": "REAL (Treasury official list)",
            "outcomes": "REAL (Zillow + ACS)",
            "validation_passed": True
        }

        return data

    def _validate_employment_data(self, data: pd.DataFrame) -> bool:
        """Validate employment data for common issues"""
        checks = []

        # Check for missing values
        checks.append(data["employment"].notna().sum() > 0)

        # Check for reasonable ranges
        checks.append((data["employment"] >= 0).all())
        checks.append((data["employment"] < 1e7).all())  # Max reasonable employment

        # Check for temporal consistency
        checks.append(data.groupby("msa")["year"].is_monotonic_increasing.any())

        return all(checks)

    def get_provenance_badge(self, dataset_key: str) -> str:
        """
        Generate markdown badge for data provenance

        Returns:
            Markdown formatted provenance disclosure
        """
        if dataset_key not in self.provenance:
            return "‚ö†Ô∏è **DATA PROVENANCE: UNKNOWN**"

        prov = self.provenance[dataset_key]
        source = prov.get("source", "Unknown")
        connector = prov.get("connector", "Unknown")
        timestamp = prov.get("timestamp", "Unknown")
        validated = "‚úÖ" if prov.get("validation_passed", False) else "‚ùå"

        badge = f"""
## üìä DATA PROVENANCE BADGE

| Attribute | Value |
|-----------|-------|
| **Data Source** | {source} |
| **Connector** | `{connector}` |
| **Last Updated** | {timestamp} |
| **Validation Status** | {validated} PASSED |
| **Data Authenticity** | ‚úÖ REAL DATA (No simulation) |

**Treatment Assignment:** REAL (from authoritative source)
**Outcome Variables:** REAL (from official statistics)
**Causal Estimates:** EMPIRICAL (not pre-programmed)
        """

        return badge.strip()
```

### Usage Example in Notebooks

```python
# In notebook header
from data_connectors.kass_data_manager import KASSDataManager

dm = KASSDataManager(cache_dir=Path("../data/cache"))

# Fetch real data
msa_employment = dm.get_msa_employment(
    msa_codes=["14460", "12060", "19100"],  # Boston, Atlanta, Dallas
    industry_codes=["10", "1012", "1013"],   # Total, Construction, Manufacturing
    years=range(2015, 2024)
)

# Display provenance badge
from IPython.display import Markdown
display(Markdown(dm.get_provenance_badge("msa_employment")))
```

---

## VII. QUALITY ASSURANCE FRAMEWORK

### Automated Data Validation

Create `KASS/tests/test_data_quality.py`:

```python
import pytest
import pandas as pd
from data_connectors.kass_data_manager import KASSDataManager

class TestDataQuality:
    """Automated tests for data quality and provenance"""

    def test_no_simulated_data(self, notebook_path):
        """Verify notebook contains no data generation code"""
        with open(notebook_path) as f:
            content = f.read()

        forbidden_patterns = [
            "np.random.normal",
            "simulate_",
            "generate_synthetic",
            "pre-programmed effect",
            "tau = 1.5"  # Specific simulated effect size
        ]

        for pattern in forbidden_patterns:
            assert pattern not in content, \
                f"Found simulation code: {pattern} in {notebook_path}"

    def test_data_provenance_present(self, notebook_path):
        """Verify data provenance badge exists"""
        with open(notebook_path) as f:
            content = f.read()

        assert "DATA PROVENANCE" in content, \
            f"Missing provenance badge in {notebook_path}"
        assert "REAL DATA" in content or "Real Data" in content, \
            f"Data authenticity not declared in {notebook_path}"

    def test_employment_data_reasonable(self, employment_df):
        """Validate employment data ranges"""
        assert (employment_df["employment"] >= 0).all()
        assert (employment_df["employment"] < 1e7).all()
        assert employment_df["employment"].notna().sum() > 0

    def test_treatment_assignment_authentic(self, treatment_data):
        """Verify treatment comes from authoritative source"""
        # Check metadata indicates real treatment
        assert "Treasury" in treatment_data.attrs.get("source", "") or \
               "CDFI" in treatment_data.attrs.get("source", ""), \
               "Treatment assignment not from Treasury"

    @pytest.mark.parametrize("notebook", [
        "notebooks/production/07-labor-market-intelligence.ipynb",
        "notebooks/production/20-opportunity-zone-evaluation.ipynb",
    ])
    def test_production_notebooks_real_data(self, notebook):
        """Production notebooks must have 100% real data"""
        # Execute notebook and check all data sources
        # (Requires papermill or nbconvert)
        pass
```

### Peer Review Checklist

Create `KASS/docs/PEER_REVIEW_CHECKLIST.md`:

```markdown
# KASS Notebook Peer Review Checklist

## Data Authenticity (Critical)
- [ ] All data sources are from authoritative APIs/files (no simulation)
- [ ] Treatment assignment comes from official records (not generated)
- [ ] Outcome variables are observed data (not calibrated simulations)
- [ ] Data provenance badge is present and accurate
- [ ] No `np.random` or `simulate_` functions in data preparation

## Methodological Correctness
- [ ] Causal identification strategy is clearly stated
- [ ] All identifying assumptions are documented
- [ ] Threats to validity are discussed with severity ratings
- [ ] Robustness checks are comprehensive
- [ ] Standard errors use appropriate heteroskedasticity correction

## Transparency
- [ ] "What This Analysis DOES" section is present
- [ ] "What This Analysis DOES NOT" section is present
- [ ] Limitations are discussed honestly
- [ ] External validity boundaries are clear
- [ ] Sensitivity analysis results are reported

## Reproducibility
- [ ] All data fetching code is included
- [ ] API credentials handling is documented
- [ ] Environment specification is current
- [ ] Code executes without errors
- [ ] Results are deterministic (or random seed is set)

## Operational Safety
- [ ] Executive summary accurately reflects data provenance
- [ ] Effect sizes are not overstated
- [ ] Policy recommendations are appropriately cautious
- [ ] Citation guidelines are provided
- [ ] Intended use cases are specified

## Documentation
- [ ] Production Data Requirements section is complete
- [ ] Data connector usage is documented
- [ ] Changelog tracks major updates
- [ ] README explains notebook status (production vs. template)

## Grade Certification
- [ ] Reviewer certifies: Grade A- or above (90+/100)
- [ ] Approved for production use: YES / NO
- [ ] Recommended actions before publication: [List]

**Reviewer Name:** ___________________
**Date:** ___________________
**Signature:** ___________________
```

---

## VIII. IMPLEMENTATION TIMELINE

### Month-by-Month Breakdown

**Month 1: Infrastructure & NB07 Start**
- Week 1: Repository reorganization, badge creation
- Week 2: Data connector setup, API credentials
- Week 3: NB07 data acquisition (BLS QCEW/LAUS)
- Week 4: NB07 data integration

**Month 2: NB07 Completion**
- Week 5: NB07 analysis update
- Week 6: NB07 QA and peer review
- Week 7: NB07 documentation
- Week 8: Buffer / catch-up

**Month 3: NB07 Finalization**
- Week 9: NB07 final validation
- Week 10: NB07 publication prep
- Week 11: Start NB20 (Treasury OZ data)
- Week 12: NB20 data architecture

**Month 4: NB20 Data Acquisition**
- Week 13-14: Zillow/ACS data fetching
- Week 15-16: Tract-level dataset construction

**Month 5: NB20 Analysis**
- Week 17-18: DiD re-estimation
- Week 19-20: Visualization updates

**Month 6: NB20 Completion**
- Week 21-22: NB20 QA and peer review
- Week 23-24: NB20 finalization

**Months 7-8: NB22 DUA Negotiation**
- Identify state partners
- Submit DUA applications
- IRB protocols if needed

**Months 9-10: NB22 Data Acquisition**
- Receive state PIRL data
- Build secure data environment
- Data quality assessment

**Months 11-12: NB22 Analysis & Finalization**
- Re-estimate treatment effects
- Update ROI calculations
- Peer review and publication

---

## IX. RESOURCE REQUIREMENTS

### Personnel
- **Lead Analyst:** 0.5 FTE (entire project)
- **Data Engineer:** 0.25 FTE (Months 1-3)
- **Statistical Reviewer:** 0.1 FTE (QA throughout)
- **Legal/Compliance:** 0.1 FTE (DUA negotiations, Months 7-8)

### Compute Resources
- **API Credits:**
  - BLS: Free (public use)
  - Census: Free (public use)
  - FRED: Free (public use)
  - Zillow: Evaluate pricing (or use free public data)
- **Storage:** ~50GB for cached datasets
- **Computing:** Standard workstation sufficient

### Software
- ‚úÖ krl-data-connectors (already available)
- ‚úÖ pandas, numpy, scipy (standard stack)
- ‚úÖ statsmodels, econml (causal inference)
- ‚ö†Ô∏è Optional: Secure data environment for PIRL (if state data acquired)

### Budget Estimate
- **Personnel (12 months):** $80,000-$120,000
- **Data subscriptions:** $0-$5,000 (if Zillow required)
- **Compute/storage:** $0-$1,000
- **Legal review:** $2,000-$5,000
- **Total:** $82,000-$131,000

---

## X. SUCCESS METRICS

### Quantitative Targets
| Metric | Current | Target | Timeline |
|--------|---------|--------|----------|
| Notebooks with real data | 0/6 | 3/6 | Month 6 |
| Overall repository grade | 83/100 | 95/100 | Month 12 |
| Data authenticity score | 20/100 | 95/100 | Month 12 |
| Production-ready notebooks | 0 | 3 | Month 12 |

### Qualitative Indicators
- ‚úÖ Peer-reviewed publications accepted
- ‚úÖ Federal agency citations
- ‚úÖ No misattribution incidents
- ‚úÖ Positive user feedback on data quality

---

## XI. RISK MITIGATION

### Risk 1: Data Access Delays
**Probability:** HIGH (especially state PIRL)
**Impact:** MODERATE (timeline slippage)
**Mitigation:**
- Pursue parallel paths (state DUA + public-use data)
- Have fallback to USDOL aggregates for NB22
- Front-load easier notebooks (NB07, NB20)

### Risk 2: API Rate Limits
**Probability:** MEDIUM
**Impact:** LOW (minor delays)
**Mitigation:**
- Implement caching to minimize re-fetches
- Respect rate limits in connectors
- Batch requests efficiently

### Risk 3: Zillow Data Unavailability
**Probability:** MEDIUM
**Impact:** MODERATE (NB20 outcomes less granular)
**Mitigation:**
- Use ACS median home values (lower frequency but free)
- Consider CoreLogic subscription if budget allows
- Aggregate ZIP ‚Üí tract with crosswalks

### Risk 4: Results Differ from Simulation
**Probability:** HIGH (expected)
**Impact:** POSITIVE (more credible findings)
**Mitigation:**
- Embrace differences as validation of method
- Document simulation vs. reality comparison
- Use as teaching moment for calibration limits

---

## XII. COMMUNICATION PLAN

### Internal Stakeholders
- **Weekly:** Progress updates to team
- **Monthly:** Demonstrations of updated notebooks
- **Quarterly:** Executive briefings on grade progress

### External Stakeholders
- **Month 3:** Announce NB07 real data upgrade
- **Month 6:** Publish methodology paper on data integration
- **Month 12:** Launch "Production" notebook collection

### Documentation Updates
- Update README.md with real data status
- Add "Real Data Roadmap" to repository
- Create "Migration Guide" for other analysts
- Publish blog posts on data integration lessons

---

## XIII. NEXT STEPS (Starting Immediately)

### Day 1-3: Repository Setup
1. Create production/templates/development directories
2. Move current notebooks to templates/
3. Add warning badges to all template notebooks
4. Create data_connectors/ module structure

### Day 4-7: Data Connector Testing
1. Install krl-data-connectors package
2. Configure API credentials
3. Test each connector with sample requests
4. Document authentication patterns

### Week 2: NB07 Data Acquisition
1. Identify target MSAs for analysis
2. Fetch BLS QCEW employment data
3. Fetch BLS LAUS unemployment data
4. Validate and cache data

### Week 3-4: NB07 Integration
1. Replace simulated metro employment with real QCEW
2. Update visualizations
3. Re-run econometric models
4. Validate results

---

## XIV. CONCLUSION

This plan systematically addresses the critical data authenticity deficiency identified in the institutional audit. By leveraging the KRL data connectors infrastructure, we can upgrade KASS notebooks from methodological templates (B grade) to production-ready policy analysis (A grade) within 6-12 months.

The phased approach prioritizes quick wins (NB07), followed by medium-complexity upgrades (NB20), and concludes with high-value administrative data integration (NB22). Each phase builds on prior infrastructure and delivers incremental value.

Upon completion, KASS will be the **gold standard for transparent, reproducible policy analysis** with:
- ‚úÖ 100% real data from authoritative sources
- ‚úÖ Full data provenance documentation
- ‚úÖ Grade A (95+/100) institutional certification
- ‚úÖ Suitable for peer-reviewed publication
- ‚úÖ Acceptable for federal agency citation

**This plan is actionable, resource-scoped, and aligned with institutional quality standards.**

---

**Plan Author:** Claude Sonnet 4.5
**Audit Reference:** KASS Institutional Audit Report (2026-01-13)
**Status:** Ready for Implementation
**Next Review:** Month 3 (Progress Check)
